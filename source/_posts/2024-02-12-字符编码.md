---
title: C++中的字符编码（一）
layout: post
date: 2025-02-12
---

## 引言

从我开始学习C++到现在的几年中，出于不同的目的，如学习语言语法、练习算法、学习工程组织构建、编写项目等，我在各种不同的操作系统下，使用不同的工具链和IDE编写C++代码，几乎使用过市面上所有流行的工具链和IDE，同时在对C++生态不够熟悉的时候深受其生态混乱之害。其中C++的字符编码问题给我留下了很深的印象。不同于在其他流行的编程语言中一般都有一套规定（或约定俗成的规则）来处理字符编码问题，C++对从源码、编译到字符串类型、编码转换长期缺乏规定，标准委员会对编译器实现控制较弱，各个不同平台之间也缺乏约定，导致C++编码问题较多，乱码问题频发。

这一系列文章将围绕C++的字符编码问题展开，目前共规划三篇；

在第一篇中，我主要围绕标准的规定的程序行为说明工具链对编码的处理和乱码可能发生的原因；第二篇中，我将通过介绍C++标准库和其他流行的“大”库中的相关设施来说明C++对于字符编码的支持，总结性地说明如何规避乱码问题，并说明C++在跨平台项目中如何规范字符编码；第三篇中，我将详细讨论常见的字符编码的技术细节和编码转换的方法；

## 字符和字符集

### 字符和字符集概念

众所周知，计算机只能处理二进制串，在计算机中要对任意一个概念进行表示、计算时，要将这个概念按照一定的规则转换为一定长度的二进制串，字符和字符串也是如此，**字符集**就是这样的一个映射，将每一个字符数值映射到一个数值上，称为字符的**码点**然而，一个数值仍只是一个抽象的数学概念，并不是其二进制表示，所以接下来，字符对应的数值进一步通过**字符集编码**将其映射到计算机中的二进制表示上；然而，字符集和字符集编码是人为规定的，所以在不同的字符集和字符集编码中，同一个字符的二进制表示是不同的，在对字符进行读、写，一定要保证读写时使用的字符集和字符集编码一致或至少兼容，否则就不能产生预期的结果，最坏的情况是产生一些完全不能辨识的符号组合，也就是乱码。

### 常见字符集

#### ASCII

作为每一个编程初学者最早接触的字符集，ASCII（美国信息交换标准代码）是最基本的字符集，它面向英文字母和常见符号，共定义了128个字符，其中包含英文字母、符号、不可见的控制字符等，是常见环境中支持最广泛的字符集，也是字符集历史中最原始的字符集，其不仅是一个独立的字符集，而且还被其它国家的字符集以ASCII扩展字符集的形式支持，绝大多数国家的专有字符集都与ASCII兼容，这赋予了ASCII无与伦比的兼容性和在不同字符集环境中移植的能力。

ASCII同时也是字符集编码。

#### ANSI

所谓ANSI字符集并不是指某一种特殊的字符集。它实际上是指不同国家根据自己需求所定义的字符集的总称。 就比如中国的GB2312(GBK、GB18030)和BIG5，日本的Shift-JIS，以及其他国家定义的ASCII扩展字符集，这些字符集往往互不兼容，在一个给定字符集的环境中，使用不同ANSI字符集的文本往往不能被很好的支持，这样的局面直接促使了Unicode的诞生。

#### Unicode

在全世界出现了很多不同的ANSI字符集之后，很多国家都在为自己的文字编码，并且互不想通，这就造成不同的语言字符编码值相同却代表不同的符号（例如：韩文编码EUC-KR中“한국어”的编码值正好是汉字编码GBK中的“茄惫绢”）。因此，同一份文档，拷贝至不同语言环境的机器，就可能成了乱码。所以，全球字符大一统的需求应运而生。

首先需要明确一点的是Unicode仅仅是一个字符集，UTF-8、UTF-16、UTF-32等字符编码则实现了它。这个字符集特别大，大到足以容纳全世界所有的字符。从而可以让全世界都统一使用同一套字符集，并且可以让全世界都使用相同的编码格式(UTF-8/16/32)，从而解决乱码的问题。

关于Unicode的几种编码方式的实现细节以及它们之间、它们和其它ANSI字符集编码之间的转换，不是这一系列文章的主题，但是在后面的部分，可能会不可避免的提起。

##  程序生命周期中的字符串转换

### 一个例子

在上一节中，我提到，字符是否乱码的关键，在于字符编码成的二进制串是否能在其被使用的每个阶段都按照预期转换，事实上，从源代码到二进制文件，再到编程语言中字符串的内部表示，再到输出的文本文件、终端或被绘制到UI上的文本，这种转换无处不在。例如，我现在在编译运行一段简单的hello world程序，开发环境是：

* Windows 10 22H2 x64
* 使用zh_CN.GBK（zh_CN.windows-936）环境
* 使用MSVC工具链
* 使用CLion，默认创建的源代码文件的字符集编码为UTF-8，不带BOM

这段代码也非常简单：

```c++
#include <iostream>

int main() {
    char const *text = "你好";
    std::cout << text << std::endl;
    return 0;
}
```

如图：

<img src="C:\Users\Mand\Desktop\编码\hello_world.png"  />

这段代码足够简单，仅仅是存储并且打印了一个字符串字面量而已，然而，这段代码也可能在特定的情况下表现出乱码现象。首先，我点击界面上的调试按钮，这会编译这段代码并在调试器环境下运行，如下图所示：

![](C:\Users\Mand\Desktop\编码\hello_world_debug.png)

这段代码在调试器控制台上正确地打印出了字符串“你好”，没有任何问题，但是如果点击运行按钮旁边的运行按钮，在系统终端中运行这个程序，问题就出现了，如下图：

![](C:\Users\Mand\Desktop\编码\hello_world_run.png)

要知道为什么同样的程序为何表现出了不同的行为，就要知道从C++源代码到控制台，这段字符串发生了什么变化。

### 字符转换过程

要了解中间发生的事情，就要从编译流程说起，像main.cpp这样的C++源代码文件，被输入到编译器进行编译，而编译器需要读取源代码文件中的内容，进行检查，如果没有语法错误，编译器会将其转换为符合标准语义的低级代码（硬件相关的二进制指令），而源代码中的各类字面量，则被编译器读取并一起存储到可执行文件中，以备由指令读取使用。

一般来说，编译器会保证存储到二进制文件中的字符是使用某种字符集编码进行编码的，这个概念称为**执行字符集**；在代码

```C++
std::cout << text << std::endl;
```

中，作为参数传递给流插入运算符的字符串就是使用执行字符集编码的字符串“你好”。

对于执行字符集的规定，最早来自于C语言，C语言明确规定了编译器需要支持的、可以用于编译阶段的源代码表示的一些基础字符的集合，称**基本执行字符集**[^1]，这一字符集包含了基本的ASCII字符、数字和一些控制字符的编码定义，然而，我们今天可以在源代码中使用的远非ASCII字符集所能覆盖，因此，C编译器工具链都不同程度地提供了对来自世界各国的ANSI编码的支持，称**扩展执行字符集**[^2]，扩展执行字符集是非标准、实现定义的，编译器工具链可以决定自己提供的扩展执行字符集实现和提供方式，我们所说的执行字符集正是基本执行字符集和扩展执行字符集之和。

延续了C语言对于执行字符集的规定，C++标准对于执行字符集亦有独立的定义[^3]，其大致延续了C语言对于**基本执行字符集**的规定，并明确提及了**执行字符集**的概念，即**基本执行字符集**与额外支持的字符集（扩展执行字符集），再加上**基本宽执行字符集**，并明确指出额外支持的字符集（扩展执行字符集）基于区域设置。

对应的，由源代码文件输入到编译器、在纯文本处理阶段使用的字符集则被称为**源字符集**。

在C语言中，与源字符集的相关的标准概念是**基本字符集**或**基本源字符集**，“基本”这个词的出现意味着标准定义的这一字符集可能只包含一些基本的ASCII字符，与执行字符集类似的，我们使用的源字符集实际上包含了基本源字符集和扩展源字符集两部分，后者是由实现定义的。

如果源字符集和执行字符集不相同，编译器就会使用适当的字符集转换算法将这些字面量由源码字符集转换为执行字符集，再储存在二进制文件中。

类似的，源代码文件保存在存储设备中时使用的字符编码为**输入文件字符集**或者**源代码字符集**（注意不要和源字符集混淆），这也是源代码文件的实际编码，输入文件字符集（源代码字符集）不是标准概念，标准没有规定输入字符集是什么。

上述三者的关系如下图所示：

![](https://fulllink.s3.bitiful.net/charset.svg)

由于没有定义扩展字符集，所以编译器对上述三种字符集的转换亦没有明确规定，只是说明了转换发生的时机和处理字符的基本原则。标准所定义的**翻译阶段**[^4]，指的是一个C++程序由源代码转换为二进制程序的步骤，其中，输入文件字符集到源字符集的转换定义在第一阶段，源字符集到执行字符集的转换定义在第五阶段。

其中，第一阶段的转换要求：

* 编译器可以接受的输入文件字符集是实现定义的；
* 由输入文件字符集转换为源字符集的方式是实现定义的；
* 不在基本源字符集中的字符转换为Unicode码点表示或实现定义的其他中间形式。

第五阶段的转换要求：

* 所有字面量字符/字符串从源字符集转换为执行字符集，字符字面量类型定义了这一映射过程；
* 转义字符序列和Unicode码点序列在这一过程中转换为执行字符集的单或多字节表示；

其中，字符字面量类型的语法和语义要求如下：

```c++
char const *text = "你好";
wchar_t const *text = L"你好";
char16_t const *text16 = u"你好";
char32_t const *text32 = U"你好";
char8_t  const *text8 = u8"你好";
```
这些字面量类型和编码的对应关系是：

|   类型   |    字面量前缀    |                字符集编码                |
| :------: | :--------------: | :--------------------------------------: |
|   char   | u8（C++20前）/无 |             ASCII/ASNI/UTF-8             |
| wchar_t  |        L         | UTF-32（Unix like）/UTF-16（Windows NT） |
| char16_t |        u         |                  UTF-16                  |
| char32_t |        U         |                  UTF-32                  |
| char8_t  |  u8（C++20起）   |                  UTF-8                   |

由上可知，C++程序编译所涉及的三个字符集，标准均没有明确规定，特别是处理非基本字符所必需的扩展源字符集和扩展执行字符集；乱码正是各个编译器工具链对扩展字符集处理的不一致导致的。

> [!NOTE]
>
> 自C23和C++23起，标准对术语和实现的要求均有所变化，上述简化的内容可能不能准确反映自C23和C++23起的标准要求，但是字符集处理的基本概念和流程没有明显变化，在一定程度上，如此描述是可行的；

### 工具链行为

上节已经提到，处理字符集的方式是由实现定义的，下面列举两种常见的工具链行为，分别是Windows上的MSVC和Unix-like上的GCC，此处不讨论移植工具链和交叉编译的情形。

#### MSVC

MSVC对字符集的处理遵循以下规则，虽然没找到非常准确的参考资料，但是仍然可知，最晚从VS2015（工具链版本v14x）起，MSVC有以下行为：

* 如果没有任何自定义编译选项和指令宏进行干涉，MSVC总是使用当前操作系统的本地环境给出的编码方式作为执行字符集；
* 如果没有任何自定义编译选项和指令宏进行干涉，MSVC总是使用当前操作系统的本地环境给出的编码方式作为源字符集；除非源代码文件带有BOM标志，这时MSVC会使用UTF-8作为源字符集；
* MSVC总是认为输入字符集和源字符集是一致的；
* `/source-character=${CodePage}`及其有类似语义的指令和宏会使MSVC以`CodePage`作为输入字符集和源字符集，其中，`CodePage`可用的值与微软支持的代码页[^5]有关；
* `/execute-character=${CodePage}`及其有类似语义的指令和宏会使MSVC以`CodePage`作为执行字符集，其中，`CodePage`可用的值与微软支持的代码页[^5]有关；
* 通过上述规则明确三种字符集后，MSVC会进行适当的字符集转换，然而，MSVC一定不会进行输入字符集到源字符的转换，因为它总认为输入字符集和源字符集是相同的；

需要注意到，作为Windows平台的专用编译器，MSVC的行为在很大程度上依赖于Windows的机制，由于Windows国际化需求的出现早于Unicode和UCS项目，所以早期的Windows为了支持多个国家和地区的语言和字符集编码，设计了沿用至今的代码页机制；在Unicode和UCS项目的早期，Windows选择了UCS-2（即UTF-16的前身）作为Windows NT内核编码，但是双字节固定编码最终被确认不能满足Unicode字符集编码的要求，其直接后代UTF-16也因此改为变长编码，但是Windows内核却再也没有修改的机会了，最终UCS-2（UTF-16）被确认为Windows的内核字符集编码，Windows的两套API也源于这个时期。

需要补充说明的是，UTF-16实际上并不符合C++语言标准对于宽字符集编码的要求[^6][^3]，这是因为UTF-16并不兼容ASCII字符集，因此它也并非是基本字符集的超集，但由于它已经成为了Windows平台的事实标准，且对Windows平台的ABI产生了深远的影响，所有并没有办法修改它。

在Windows 10时期，Windows对于UTF-8的支持更进一步[^11]，系统的时间和区域设置提供了选项，允许用户使用UTF-8实验功能，这一选项会使用UTF-8代替当前的ANSI编码，这一选项也会改变Win32 API的行为，使得Windows将所有多字节字符串视为UTF-8；

然而，由于兼容性原因，许多软件并未对这一特性进行适配，大量Windows软件依赖硬编码的ANSI字符串工作，这可能导致部分软件在开启这个特性后乱码；另一些软件依赖代码页来辨识用户的国家和地区（虽然这并非代码页的真正用途），UTF-8不带有任何的区域信息，这会使这些软件无法工作；有些软件在处理单/双/多字节编码时仅仅假定其为单/双字节字符集编码，而未考虑多字节字符集编码的情况，这可能导致一些内存访问问题[^7]；同时对ANSI和UTF-8字符集编码提供支持也影响了Windows对字符文件编码的预设，利用BOM标志来判断文件是否为UTF-8正是出于这一原因。

总之，Windows平台的Unicode支持任然任重而道远。

#### GCC

MSVC对字符集的处理与MSVC不尽相同，GCC有以下行为：

* GCC总是使用UTF-8作为源字符集[^6]；
* GCC默认使用UTF-8作为执行字符集，除非使用了**-fexec-charset=charset**选项[^8]；
* GCC默认使用的执行宽字符集，是UTF-32BE、UTF-32LE、UTF-16BE、UTF-16LE之一，最终的结果与代码生成目标的字节序、**wchar_t**的宽度要求有关，除非使用了**-fwide-exec-charset=charset**选项[^9]；
* GCC会使用当前操作系统环境下的**iconv**将代码编译的中间产物中的字符由源字符集编码转换为执行字符集编码或执行宽字符集编码，除非执行字符集或执行宽字符集不受它支持，或指定的执行宽字符集不符合**wchar_t**的要求（UTF-16除外）[^9][^6]；
* GCC从系统Locale获取输入字符集编码，如果未指定或者不能访问Locale，则置为UTF-8[^10]；
* GCC会使用当前操作系统环境下的**iconv**将源代码由输入字符集编码转换为源字符集编码，除非**iconv**不可用或输入字符集编码和源字符集编码不受它支持[^10]；

可以看到，作为对兼容性没有那么重视的Unix-Like平台的工具链，GCC对Unicode的支持比较彻底，在内核方面使用了UTF-32，同时UTF-32也是平台定义的宽字符集，完全符合C++标准对于宽字符的要求，同时也直接使用了UTF-8作为默认的源字符集和执行字符集，这给字符集的处理带来的便利；同时，GCC也采取了固定源字符集的处理方法，这似乎更符合直觉，不过，虽然GCC明确表示可以使用**iconv**支持的任意字符集作为输入字符集，但是似乎在混合使用不同编码的源代码文件方面还存在一些其他问题[^12]，所以最好还是使用UTF-8作为源代码的字符集比较好。

### 运行时的字符编码转换

然而，执行编码的二进制字符表示仍不是我们使用的最终产物，在平常编码中，我们一般使用字符串字面量初始化一个字符串类，而字符串类才是我们真正使用的字符串实现，而字符串类本身也会选择其内部字符的编码方式，不同字符串类的内部编码可能是不同，下表中列出了标准库和Qt的字符串内部编码：

| 类名                     | 定义                                                         | 字符类型  | 编码                             |
| ------------------------ | ------------------------------------------------------------ | --------- | -------------------------------- |
| std::string              | std::basic_string<char, std::char_traits\<char> , std::allocator\<char>> | char      | ASCII/ASNI/UTF-8                 |
| std::wstring             | std::basic_string<wchar_t, std::char_traits\<wchar_t> , std::allocator\<wchar_t>> | wchar_t   | UTF-32/UTF-16(Unix like/Windows) |
| std::u8string（C++20起） | std::basic_string<u8char_t, std::char_traits\<u8char_t> , std::allocator\<u8char_t>> | u8char_t  | UTF-8                            |
| std::u16string           | std::basic_string<u16char_t, std::char_traits\<u16char_t> , std::allocator\<u16char_t>> | u16char_t | UTF-16                           |
| std::u32string           | std::basic_string<u32char_t, std::char_traits\<u32char_t> , std::allocator\<u32char_t>> | u32char_t | UTF-32                           |
| QString                  | QString                                                      | QChar     | 16bit字符编码                    |

可以看到，标准库字符串的内部字符表示依赖于模板参数，它们不会在构造时作任何转换，仅仅是将初始化其使用的右值或参数复制到了它分配的堆上内存空间而已；而Qt的字符类型QChar使用一种16bit的字符编码，在Qt4.x的文档中并没有说明这种编码是什么，在5.x和6.x的文档中则明确说明其为UTF-16；QString提供了多种方式当从常见的多字节字符或平台宽字符编码转向其内部编码，这些方法很灵活，但可能被错误地使用。

综上所述，一段文本从字符字面量开始，最终转换为某种字符串类型的内部表示，实际上经过了源代码字符编码、执行字符集再到字符串类内部转换的，下表展示了从源代码到运行时的字符串类内部表示过程中的各个阶段和可能影响字符编码的要素：

|                    过程                    | 发生的时机 |                      可能影响行为的要素                      |
| :----------------------------------------: | :--------: | :----------------------------------------------------------: |
|          源代码到字面量二进制表示          |   编译时   | 指定源字符集的宏或编译器指令、指定执行字符集的宏或编译器指令、编译机器的本地locale设置、字面量类型、源代码的编码类型、源代码的实际编码 |
| 字面量二进制表示到字符串类型内部二进制表示 |   运行时   |               字符串类型的构造行为、执行字符集               |

### 文本输出对于编码的要求

到现在，确定字符串内部二进制表示是否正确的办法已经非常明了了，通过检查每一个可能出现的转换和转换的输入和方法，我们可以逐一确认来自源代码的字符串字面量是否正确地转换为了字符串类型的内部表示，然而，存储一个字符串的最终目的是将字符串以某种方式输出出来，如绘制在GUI上（如我们使用的带有图形界面的软件那样）或输出到文本设备（一个泛文本文件，就像上面例子中那样），这些输出同样对于字符编码有着特定要求，在不同的上下文中，这些要求的变化可能导致相同的二进制程序在不同的环境下得到不同的执行结果。

从字符串类型内部表示到屏幕上的基本流程是（以freeType为例）：

![](https://fulllink.s3.bitiful.net/char-display.svg)

在这个过程中，字符编码值起到了索引的作用，因此要保证编码值的正确性，一般来说，图形库封装了这个过程，提供了绘制字符类图元的接口，图形库会保证将绘制图元接口传入的字符串正确的转换为字形库接口要求的字符编码，所以在编写代码时，只需要注意在使用绘制图元接口或更高级别的相关接口时保证传入的字符串编码正确即可。

在上述的例子中，之所以同样的代码会在运行和调试时一个正确一个错误，正是因为CLion在“运行”时使用操作系统的原生终端，其表现出来的行为和CMD、PowerShell一致，默认使用操作系统指定的ANSI字符集（中文系统为GBK），而“调试”功能使用的调试器控制台要求输出字符串类型为UTF-8，与windows操作系统的区域和语言设置无关，所以使用特定字符的一段代码可能无法在原生终端和调试控制台上同时表现出正确的行为，调试控制台和系统终端行为上的区别也体现在一些其他功能上，例如，调试控制台可以无法支持TUI的行为，调试TUI程序时可能出现渲染错误和无法响应鼠标事件等现象。

然而，有一些手段可以在调试控制台上模拟系统终端，这可以统一运行和调试时对编码的要求，还可以让调试控制台表现出一些原本不支持的行为，例如CLion就为此提供了设置项，当然，这个功能有一点限制[^13]；

### 回到开始的地方

到这里，终于可以解释最初提到的例子了的现象了，首先，代码编译运行的环境是windows，使用windows-936（GBK）Locale，不使用任何编译选项，源文件编码为UTF-8（无BOM），所以MSVC认为输入字符集、源字符集和执行字符集都是GBK；但是注意，例子中的实际源文件编码是UTF-8，但是因为编译器认为输入字符集、源字符集和执行字符集都是GBK，所以编译器实际上没有对输入的源代码做任何转换，所以最终的实际执行字符集其实是UTF-8；

紧接着，程序开始执行，进入运行时，`text`正确的初始化，并且指向了常量段的字符数组，接着，使用这个字符指针调用流插入运算符，字符数组中的数据被复制到`std::cout`的受控序列缓冲区，标准输出流立即刷新它的缓冲区，缓冲区中的数组写入`stdout`设备，最终通过系统调用写入到控制台设备，这整个过程中不涉及任何编码转换，所以字符串的数据一直是UTF-8；

在控制台端，处理则有些差异，如上文所属，CLion运行功能使用的是系统终端，受Locale影响，要求写入的字符是GBK编码，但实际上是UTF-8，所以控制台无法正确解码；反之，调试控制台要求UTF-8编码，所以可以正常解码显示；

## 参考资料

[^1]: [字符集与编码（C）] https://cppreference.cn/w/c/language/charset

[^2]: [MSVC定义的执行字符集] https://learn.microsoft.com/zh-cn/cpp/c-runtime-library/locale-names-languages-and-country-region-strings?view=msvc-170

[^3]: [字符集与编码（C++）] https://cppreference.cn/w/cpp/language/charset

[^4]: [Phases of translation] https://cppreference.cn/w/cpp/language/translation_phases

[^5]: [代码页标识符] https://learn.microsoft.com/zh-cn/windows/win32/Intl/code-page-identifiers
[^6]: [GCC Character sets]https://gcc.gnu.org/onlinedocs/cpp/Character-sets.html
[^7]: [为什么不能把UTF-8设为默认的ANSI代码页] Windows编程启示录（The Old New Thing - Practical Development Throughout the Evolution of Windows Raymond Chen）219页
[^8]: [exec-charset] https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html#index-fexec-charset
[^9]: [wide-exec-charset] https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html#index-fwide-exec-charset
[^10]: [input-charset] https://gcc.gnu.org/onlinedocs/gcc/Preprocessor-Options.html#index-finput-charset
[^11]: [Windows的Unicode支持] https://en.wikipedia.org/wiki/Unicode_in_Microsoft_Windows
[^12]: [How should I use g++'s -finput-charset compiler option correctly in order to compile a non-UTF-8 source file?] https://stackoverflow.com/questions/71610601/how-to-build-icu-data-dll-in-windows
[^13]: [terminal-in-the-output-console] https://www.jetbrains.com/help/clion/terminal-in-the-output-console.html



